{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "tutorial: https://www.youtube.com/watch?v=kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from tokenizer import SimpleTokenizer\n",
    "torch.manual_seed(1337)\n",
    "from dataset import SpeechesClassificationDataset\n",
    "from tokenizer import SimpleTokenizer\n",
    "from transformer import CompositeEmbedding, CustomTransformerEncoder\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt', download_dir=\"../data/nltk_punkt\")\n",
    "# nltk.data.path.append(\"../data/nltk_punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7335, 0.9315, 0.7578, 0.7938],\n",
       "         [0.2683, 0.2031, 0.2434, 0.3224],\n",
       "         [0.7478, 0.7225, 0.6855, 0.7379]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.rand((1, 3, 3))\n",
    "t2 = torch.rand(size=(1, 4, 3))\n",
    "torch.bmm(t1, t2.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "torch.arange(x.size(-1)).expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/speechesdataset/train_LM.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "# scd = SpeechesClassificationDataset(tokenizer=tok, file_path=\"data/train_CLS.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = SimpleTokenizer(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input: batch size of 3, sequence length of 5\n",
    "\n",
    "vocab_size = tok.vocab_size  # Example vocabulary size\n",
    "n_embed = 64    # Example embedding dimension\n",
    "embedding = CompositeEmbedding(vocab_size, n_embed)\n",
    "\n",
    "sample_indices = torch.randint(0, vocab_size, (3, 5)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2864, 4029, 4024, 2205, 4007],\n",
       "        [1656, 1845, 2248, 2023, 3120],\n",
       "        [ 621, 3543, 2456, 2936, 3478]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "e= embedding.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = e(sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tok.vocab_size\n",
    "n_embed = 64       \n",
    "n_heads = 2       \n",
    "n_layer = 4       \n",
    "n_input = 64      \n",
    "n_hidden = 100  \n",
    "n_output = 3 \n",
    "batch_size = 16\n",
    "seq_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomTransformerEncoder(\n",
       "  (embedding): CompositeEmbedding(\n",
       "    (token_embeddings): Embedding(4526, 64)\n",
       "    (positional_embeddings): Embedding(4526, 64)\n",
       "    (normalize): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_layers): ModuleList(\n",
       "    (0-3): 4 x NHeadAttention(\n",
       "      (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (normalize): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ffnet): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomTransformerEncoder(vocab_size, n_embed, n_heads, n_layer, n_input, n_hidden, n_output).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_length), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 114, 4241, 2757, 2751, 2242, 2215, 2749, 2657, 3674, 3088],\n",
       "        [2347, 4407, 1953, 4194, 1541, 4301,  200, 2172, 1281, 1181],\n",
       "        [2027, 2721, 4017, 3374, 2137,  357, 3910, 3827,  247, 1339],\n",
       "        [1522, 1857,  934, 1695, 3293,  919,  892, 3648, 4262, 3405],\n",
       "        [1525,  639, 2941, 3802, 3506,  380, 1274, 1817, 3798, 4470],\n",
       "        [ 911, 1958, 3822, 1788, 4084, 2874,  437, 3616, 4366, 3618],\n",
       "        [ 144, 2560, 3064, 2810, 3198,  387, 2399,  999, 1077,  338],\n",
       "        [ 106, 1333,  955, 2622, 2010, 2753, 3636, 1735, 2454, 3344],\n",
       "        [2305, 2334, 2190, 3269, 4040, 1261,  215, 2282, 2630, 1529],\n",
       "        [2582, 3653, 1921, 2019, 3587, 3724, 4105, 3925, 2662, 2865],\n",
       "        [ 319,  868, 3815, 2109, 2796, 3295, 4194, 1052, 1158, 1300],\n",
       "        [2973, 3971,  502, 4099, 1038, 4457, 2095,  716, 1967, 2196],\n",
       "        [2040, 4346, 4360, 4156, 2789, 1783, 4480, 1426, 3215, 1591],\n",
       "        [4426, 2265, 2589, 3506, 2359, 2071, 2067,  957, 2141, 1622],\n",
       "        [1983, 1778, 2687,  490, 4221, 4286, 4414, 1299, 1365, 3524],\n",
       "        [1325, 3783, 4390, 2445, 3088, 1107, 2791, 3331, 2307, 1940]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0009, -0.5531,  0.2170],\n",
       "        [-0.0300, -0.2346,  0.0526],\n",
       "        [-0.0867, -0.3334, -0.0536],\n",
       "        [ 0.0575, -0.2837,  0.2274],\n",
       "        [ 0.1406, -0.5387,  0.1170],\n",
       "        [ 0.0458, -0.3529,  0.2439],\n",
       "        [-0.0666, -0.3641,  0.2693],\n",
       "        [ 0.0775, -0.3704,  0.0687],\n",
       "        [ 0.1858, -0.4392,  0.2118],\n",
       "        [ 0.0761, -0.1543,  0.1010],\n",
       "        [ 0.0364, -0.5550, -0.0031],\n",
       "        [ 0.2411, -0.0917, -0.1485],\n",
       "        [-0.0159, -0.1915,  0.1488],\n",
       "        [ 0.0188, -0.3322,  0.3497],\n",
       "        [ 0.1658, -0.3319, -0.0485],\n",
       "        [-0.1397, -0.0781,  0.1208]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = 42\n",
    "\"\"\" Hyperparameters to use for training to roughly match \n",
    "the numbers mentioned in the assignment description \"\"\"\n",
    "batch_size = 16  # Number of independent sequences  we will process in parallel\n",
    "block_size = 32  # Maximum context length for predictions\n",
    "learning_rate = 1e-3  # Learning rate for the optimizer\n",
    "n_embd = 64  # Embedding dimension\n",
    "n_head = 2  # Number of attention heads\n",
    "n_layer = 4  # Number of transformer layers\n",
    "\n",
    "\n",
    "eval_interval = 100  # How often to evaluate train and test perplexity during training\n",
    "max_iters = 500 # For language modeling, we can process all the batches for the entire dataset, but that takes a while, so we'll limit it to 500 iterations. For batch size of 16 and block size of  32, this is roughly, this is  500 * 16 * 32 = 256000 tokens, SOTA LMs are trained on trillions of tokens, so this is a very small dataset.\n",
    "eval_iters = 200  # Number of iterations to evaluate perplexity on the test set\n",
    "\n",
    "\n",
    "## classifier training hyperparameters. It is a simple 1 hidden layer feedforward network, with input \n",
    "## size of 64, hidden size of 50 and output size of 3.\n",
    "\n",
    "n_input = 64  # Input size for the classifier, should match the embedding size of the transformer\n",
    "n_hidden = 100  # Hidden size for the classifier\n",
    "n_output = 3  # Output size for the classifier, we have 3 classes\n",
    "epochs_CLS = 15 # epochs for classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and creating tokenizer ...\n",
      "Vocabulary size is 5755\n"
     ]
    }
   ],
   "source": [
    "#LOAD DATA\n",
    "import os\n",
    "from main import load_texts, DataLoader, collate_batch, LanguageModelingDataset\n",
    "data_dir = os.path.join(\"..\", \"data\", \"speechesdataset\")\n",
    "print(\"Loading data and creating tokenizer ...\")\n",
    "texts = load_texts('../data/speechesdataset')\n",
    "tokenizer = SimpleTokenizer(' '.join(texts)) # create a tokenizer from the data\n",
    "print(\"Vocabulary size is\", tokenizer.vocab_size)\n",
    "\n",
    "train_CLS_dataset = SpeechesClassificationDataset(tokenizer, os.path.join(data_dir, \"train_CLS.tsv\"))\n",
    "train_CLS_loader = DataLoader(train_CLS_dataset, batch_size=batch_size,collate_fn=collate_batch,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = CustomTransformerEncoder(vocab_size, n_embed, n_heads, n_layer, n_input, n_hidden, n_output).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable for classification tasks\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Learning rate might need tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - xb: torch.Size([16, 32]), yb: torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [183,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [117,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "../aten/src/ATen/native/cuda/Indexing.cu:1289: indexSelectLargeIndex: block: [101,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#model prediction  \u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m te(xb)  \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#compute loss\u001b[39;00m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, yb)\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev_local/cse256/hw2/src/transformer.py:108\u001b[0m, in \u001b[0;36mCustomTransformerEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_layers:\n\u001b[0;32m--> 108\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# take the mean over all the embeddings for each token\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# gives a length-indep. repr. for each sequence\u001b[39;00m\n\u001b[1;32m    111\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/prob-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev_local/cse256/hw2/src/transformer.py:65\u001b[0m, in \u001b[0;36mNHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#value is of shape [batch_size, seq_length, num_heads, head_dim]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# matmul does not need contiguous inputs\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m raw_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m     66\u001b[0m attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(raw_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "inputfile = os.path.join(data_dir, \"train_LM.txt\")\n",
    "with open(inputfile, 'r', encoding='utf-8') as f:\n",
    "    lmtrainText = f.read()\n",
    "train_LM_dataset = LanguageModelingDataset(tokenizer, lmtrainText,  block_size)\n",
    "train_LM_loader = DataLoader(train_LM_dataset, batch_size=batch_size, shuffle=True)\n",
    "    # for the classification  task, you will train for a fixed number of epochs like this:\n",
    "for epoch in range(epochs_CLS):\n",
    "    te.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_CLS_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)  # Move data to the appropriate device\n",
    "        print(f\"Batch shapes - xb: {xb.shape}, yb: {yb.shape}\")\n",
    "        #zero out grads\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"zeroed out gradient\")\n",
    "        #model prediction  \n",
    "        outputs = te(xb)  \n",
    "        print(f\"made prediction\")\n",
    "        #compute loss\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        #backprop \n",
    "        loss.backward()\n",
    "        #grad update  \n",
    "        optimizer.step()  \n",
    "\n",
    "        #update loss for this epoch\n",
    "        total_loss += loss.item()  \n",
    "\n",
    "    #print avg loss over epoch\n",
    "    epoch_avg_loss = total_loss / len(train_CLS_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs_CLS}], Loss: {epoch_avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
