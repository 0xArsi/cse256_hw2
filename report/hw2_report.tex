\documentclass[10pt]{article}
\usepackage{tocloft}
\usepackage{adjustbox}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage[normalem]{ulem}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{sectsty}
\usepackage{subcaption}
\usepackage{csvsimple}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{trees}
\usepackage{hyperref}
\hypersetup{
        colorlinks = true,
        linkcolor = blue,
        filecolor = magenta,            
        urlcolor = cyan,
        pdftitle={Overleaf Example},
        pdfpagemode=FullScreen,
}

\title{Homework 2: Classification \& Language Modeling}
\author{Isaac Thomas}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}
\setcounter{section}{0}

\newcommand{\code}[1]{\texttt{#1}}

\makeatletter
\renewcommand\paragraph{\@startsection{subparagraph}{5}{\parindent}%
        {3.25ex \@plus1ex \@minus .2ex}%
        {0.75ex plus 0.1ex}% space after heading
        {\normalfont\normalsize\bfseries}}
\makeatother

\makeatletter

\newcommand{\dateformatter}[2]{%
    \textbf{#1} -- \textit{#2}%
}

\newcommand{\dateevent}[2]{%
    \addcontentsline{dates}{section}{#1 -- #2}%
    \dateformatter{#1}{#2}%
}

\newcommand{\listofdates}{%
    \begingroup
    \renewcommand{\contentsname}{List of Dates}
    \let\old@starttoc\@starttoc
    \def\@starttoc##1{%
        \old@starttoc{dates}%
    }
    \tableofcontents%
    \endgroup
}

\makeatother

%\AddToHook{cmd/section/before}{\clearpage}
\sectionfont{\fontsize{12}{15}\selectfont}
\subsectionfont{\fontsize{10}{15}\selectfont}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\DeclareSymbolFont{matha}{OML}{txmi}{m}{it}% txfonts
\DeclareMathSymbol{\varv}{\mathord}{matha}{118}
\setlistdepth{100}
\newenvironment{myenum}%
{\pointedenum\begin{enumerate}}%
{\end{enumerate}}
\begin{document}
\maketitle

%PROBLEM 2
\section{Problem 1: Speech Classification}
\noindent For this classification problem - namely, predict which president said a given sentence - we used a transformer encoder classifier consisting of:
\begin{itemize}
    \item a composite embedding which uses \code{torch.nn.Embedding} and a sinusoidal positional encoding to map each token to a high-dimension latent representation
    \item multiple sequential \textbf{encoder} blocks, each of which uses multi-head attention to extract contextual relationships between tokens
    \item a fully connected layer producing a one-hot vector corresponding to the predicted class
\end{itemize}

\noindent The training and evaluation dataset consisted of numerous sentences, each of which was uttered by one of three presidents. We trained this model (\code{epochs=15}, \code{batch\_size}=16) on the former dataset using an adam optimizer with a learning rate of $\alpha = 0.001$. The resulting model achieved an accuracy of $86.8\%$ on the evaluation dataset. Below is a table depicting the progression of training loss/accuracy:\\
\begin{center}
\csvautotabular{../data/training_metrics/transformer_encoder.csv}
\end{center}

\noindent We accompany this data with visualized attention maps from the last of each encoder's attention heads. The corresponding plots below depict which tokens were found to have strong contextual relationships:

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.4]{../data/plots/part1/attention_map_1.png}
        \label{subfig:am1}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.4]{../data/plots/part1/attention_map_2.png}
        \label{subfig:am2}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.4]{../data/plots/part1/attention_map_3.png}
        \label{subfig:am3}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.4]{../data/plots/part1/attention_map_4.png}
        \label{subfig:am4}
    \end{subfigure}
\caption{the \code{n\_head}-th attention map from each encoder layer in the transformer encoder. The progressive acquisition of contextual relationships between the tokens in the training data is visible from the increase in cell intensity from layer to layer.}
\end{figure}
\noindent Along with the generally increasing understanding of token-wise relationships, we see that tokens have affinity towards preceding tokens; this is visible from the intensities being much stronger in the first few columns than in the first few rows. This aligns with our natural understanding of the english language, as the contextual meaning of a word is derived from preceding words in the same sentence. That being said, some acquisition of context from future tokens is also visible. We should expect not to see such a thing in the decoder model we implement due to the masking in the self-attention heads. Additionally, it seems there is abundant affinity between too many pairs of tokens. Applying dropout while maintaining the total "attention mass" one token can "pay" to other tokens could force the network to focus more on the token relationships that matter at the expense of the rest. We will discuss this more in part 3.

%PROBLEM 2
\section{Problem 2: Language Modeling}
This language modeling problem involves predicting the most likely following token based on information from previous tokens. For this task we used a transformer decoder consisting of:
\begin{itemize}
    \item a composite embedding which uses \code{torch.nn.Embedding} and a sinusoidal absolute positional encoding to map each token to a high-dimension latent representation.
    \item multiple sequential \textbf{decoder} blocks, each of which uses masked multi-head self- attention to extract contextual relationships between a given token and those preceding it
    \item a fully connected layer, which produces a one-hot vector indicating the most likely token in our vocabulary to come next \textbf{for each token in the given sequence}
\end{itemize}

\noindent The training dataset consisted of numerous sentences, each of which was tokenized and collated into batches before training like in the first problem. The test datasets were speech corpi from George H. Bush, Barack Obama, and George W. Bush respectively. We trained this model (\code{epochs=15}, \code{batch\_size}=16) on the first of these four datasets using an adam optimizer with a learning rate of $\alpha = 0.001$. We depict the training loss/perplexity every 100 iterations below.

\begin{center}
\csvautotabular{../data/training_metrics/transformer_decoder.csv}
\end{center}

\noindent We then tested the model on datasets from speeches given by George H. Bush, Barack Obama, and George W. Bush. The resulting perplexities are depicted below: 
\begin{center}
\csvautotabular{../data/testing_metrics/transformer_decoder.csv}
\end{center}
The model did not attain as low a perplexity for the H. Bush and W. Bush datasets compared to our expectations; it did, however, attain comparable perplexity on the Obama dataset. The results suggest that there is a difference in contextual ambiguity between Obama's speeches compared to the Bush speeches. It's possible that Obama uses more concise and sophisticated language with fewer or significantly clearer contextual relationships within components of each sentence spoken, resulting in lower-entropy predictions from the model.\\

\noindent The attention maps depicted below indicate the strengths of these relationships. It's clear that our masking approach forced attention to only be paid to preceding tokens since the upper triangular half of each map has zero intensity. That said, it seems that the attention heads could be more ``focused'' and distribute their attention mass less uniformly, as some contextual relationships should matter way more than others.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../data/plots/part2/attention_map_1.png}
        \label{subfig:am1}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../data/plots/part2/attention_map_2.png}
        \label{subfig:am2}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../data/plots/part2/attention_map_3.png}
        \label{subfig:am3}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../data/plots/part2/attention_map_4.png}
        \label{subfig:am4}
    \end{subfigure}
\caption{the \code{n\_head}-th attention map from each encoder layer in the trained transformer decoder. As mentioned before, we only see nonzero affinities between the $i$-th token and the $j$-th token in a given block if $i > j$ (token $j$ precedes token $i$).}
\end{figure}
\section{Problem 3: Exploration}
\subsection{Architectural Exploration}

\noindent We mentioned in part 1 that we could potentially improve the performance of the classifier by sharpening the focus of each decoder layer's attention heads. Exploring this option further, we applied dropout ($p =0.5$) to the attention weights after applying the softmax function, then re-normalized the weights so they still summed up to 1 across each row (see \code{transformer.AttentionHead}). We then trained the resulting model on the same training dataset as in part 1 and computed the same training/evaluation metrics.

\noindent As in part 1, the training and evaluation dataset consisted of numerous sentences, each of which was uttered by one of three presidents. We trained this model (\code{epochs=15}, \code{batch\_size}=16) on the former dataset using an adam optimizer with a learning rate of $\alpha = 0.001$. The resulting model achieved an accuracy of $82.8\%$ on the evaluation dataset, which was surprisingly worse than part 1. Below is a table depicting the progression of training loss/accuracy:\\
\begin{center}
\csvautotabular{../data/training_metrics/transformer_encoder_enhanced.csv}
\end{center}

\noindent We accompany this data with visualized attention maps from the last of each encoder's attention heads. The corresponding plots below depict which tokens were found to have strong contextual relationships:
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.4]{../data/plots/part3/attention_map_1.png}
        \label{subfig:am1}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.4]{../data/plots/part3/attention_map_2.png}
        \label{subfig:am2}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.4]{../data/plots/part3/attention_map_3.png}
        \label{subfig:am3}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.4]{../data/plots/part3/attention_map_4.png}
        \label{subfig:am4}
    \end{subfigure}
\caption{the \code{n\_head}-th attention map from each encoder layer in the transformer encoder. The affinities are visibly concentrated instead of being more spread out across the attention map like in part 1.}
\end{figure}
\noindent We can see here that the attention maps have more concentrated intensities in fewer areas of the map compared to part 1. This suggests that the model learned to focus more on the most influential token relationships since such mass would be dropped out randomly. Despite the lower test accuracy, it's possible that this approach could generalize better to other datasets than the model trained in part 1.\\


%Best te params: {'n_embed': 32, 'n_heads': 4, 'n_layer': 2, 'n_hidden': 100, 'p_dropout': 0, 'batch_size': 16, 'alpha': 0.0038935580035601813}
\subsection{Performance Improvement}
\noindent For performance improvment, we first attempted to tune 20 configurations of our dropout-enhanced transformer encoder over a reasonable search space using RayTune and an ASHA scheduler, which asynchronously tunes multiple model configurations and stops underperforming models early. This process was not very successful, as most of the losses became \code{nan} in later iterations of training, causing the corresponding model configurations to be terminated. This was likely due to the dropout nondeterministically zeroing out the denominator of the post-softmax renormalization, causing \code{nan} losses/gradients. The records for this run are under \code{data/tuning\_results\_transformer\_encoder\_dropout.csv}. We then performed the same tuning procedure with \code{p\_dropout=0}, which stabilized the losses and gradients. The results from this run can be found at \code{data/tuning\_results\_transformer\_encoder.csv}. The best configuration from this run was the following:
\newpage
\begin{lstlisting}[language=Python, basicstyle=\ttfamily\small, frame=single] 
    {
     'n_embed': 32,
     'n_heads': 4,
     'n_layer': 2,
     'n_hidden': 100,
     'p_dropout': 0,
     'batch_size': 16,
     'alpha': 0.0038935580035601813, 
    } 
\end{lstlisting}
when we evaluated the model on the test data

\bibliographystyle{plain}
\bibliography{references}
\end{document}