\documentclass[10pt]{article}
\usepackage{tocloft}
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{float}
\usepackage{graphics}
\usepackage{caption}
\usepackage[normalem]{ulem}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{sectsty}
\usepackage{csvsimple}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{trees}
\usepackage{hyperref}
\hypersetup{
        colorlinks = true,
        linkcolor = blue,
        filecolor = magenta,            
        urlcolor = cyan,
        pdftitle={Overleaf Example},
        pdfpagemode=FullScreen,
}

\title{Homework 2: Classification \& Language Modeling}
\author{Isaac Thomas}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}
\setcounter{section}{0}

\newcommand{\code}[1]{\texttt{#1}}

\makeatletter
\renewcommand\paragraph{\@startsection{subparagraph}{5}{\parindent}%
        {3.25ex \@plus1ex \@minus .2ex}%
        {0.75ex plus 0.1ex}% space after heading
        {\normalfont\normalsize\bfseries}}
\makeatother

\makeatletter

\newcommand{\dateformatter}[2]{%
    \textbf{#1} -- \textit{#2}%
}

\newcommand{\dateevent}[2]{%
    \addcontentsline{dates}{section}{#1 -- #2}%
    \dateformatter{#1}{#2}%
}

\newcommand{\listofdates}{%
    \begingroup
    \renewcommand{\contentsname}{List of Dates}
    \let\old@starttoc\@starttoc
    \def\@starttoc##1{%
        \old@starttoc{dates}%
    }
    \tableofcontents%
    \endgroup
}

\makeatother

%\AddToHook{cmd/section/before}{\clearpage}
\sectionfont{\fontsize{12}{15}\selectfont}
\subsectionfont{\fontsize{10}{15}\selectfont}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\DeclareSymbolFont{matha}{OML}{txmi}{m}{it}% txfonts
\DeclareMathSymbol{\varv}{\mathord}{matha}{118}
\newcommand{\Setup}{\code{Setup}}
\newcommand{\Prove}{\code{Prove}}
\newcommand{\Verify}{\code{Verify}}
\newcommand{\Sim}{\code{Sim}}
\setlistdepth{100}
\newenvironment{myenum}%
{\pointedenum\begin{enumerate}}%
{\end{enumerate}}
\begin{document}
\maketitle

\section{Problem 1: Speech Classification}
\noindent For this classification problem, we used a transformer encoder classifier consisting of:
\begin{itemize}
    \item a composite embedding which uses \code{torch.nn.Embedding} and a sinusoidal positional encoding to map each token to a high-dimension latent representation
    \item multiple sequential encoder blocks, each of which uses multi-head attention to extract contextual relationships between tokens
    \item a fully connected layer, which produces a one-hot vector corresponding to the predicted class
\end{itemize}

\noindent The training and evaluation dataset consisted of sentences, each of which was uttered by one of three presidents. We trained this model (\code{epochs=15}, \code{batch\_size}=16) on the former dataset using an adam optimizer with a learning rate of $\alpha = 0.001$. The resulting model achieved an accuracy of $83.2\%$ on the evaluation dataset. Below is a table depicting the progression of training loss/accuracy:\\
\begin{center}
\csvautotabular{../data/training_metrics/transformer_encoder.csv}
\end{center}


\section{Problem 2: Language Modeling}
\section{Problem 3: Improving Previous Models}

\bibliographystyle{plain}
\bibliography{references}
\end{document}